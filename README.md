# Data Scientist

# Technical Skills: 
## Python, SQL, R, C++
----

# Tools : 
## Power BI, Tableau, AWS, AZURE, MySQL, IBM Watson, Google Sheet, ERP(IFS), SAP, MS Excel, MS Word, MS PowerPoint, MS OutLook, VS Code, Jupyter
---

# Education

- ðŸŽ“ High school  |  Mathematics                            |  SALAM    |  Tehran, Iran  
- ðŸŽ“ Bachelor     |  Mechanical Engineering                 |  Azad Univeristy  | Tehran, Iran  |  Feb 202
- ðŸŽ“ Master       |  Industrial Engineering & International Management with a focus on Data Science and AI  |  Hochschule Fresenius  |  Berlin, Germany  |  Expected in Feb 2025

---

# Summary of Skills & Expertise ðŸš€

- ðŸ“Š Data Science: Expertise in the entire data lifecycle, including meticulous data understanding, cleaning, EDA, feature engineering, modeling, and machine learning.
- ðŸ’» Programming Languages: Proficient in Python, utilizing libraries such as seaborn, scikit-learn, matplotlib, etc. Knowledgeable in R and SQL.
- ðŸ“ˆ Visualization: A keen eye for data visualization to convey meaningful insights.
- ðŸ¤– Machine Learning: Skilled in building robust machine learning models for forecasting and decision-making.

###  Summary of projects 

- ## Project 6 : Loan Prediction
- ## Project 5 : Salary Estimation for Data Positions in the USA Public
- ## Project 4 : Landing Prediction: Falcon 9 SpaceX - Data Science Certificate by IBM
- ## Project 3 : Rain Prediction in Australia
- ## Project 2 : Analysis of House Sales in the USA - Coursera Project
- ## Project 1 : Analyzing Stock Performance and Building a Dashboard Mini Project Course
----------


- ## Project 6 : Loan Prediction
 Gathering data from an international bank dataset from 2021 involves the initial step of acquiring raw information from various sources within the banking domain. This process entails accessing databases, APIs, or other data repositories to collect relevant data points such as customer information, financial transactions, loan details, etc. 

Cleaning the data is the subsequent phase, which involves preprocessing and refining the collected data to ensure its quality, consistency, and completeness. This step often includes tasks such as handling missing values, removing duplicates, correcting errors, and standardizing formats.

After cleaning the data, the next step is data analysis and exploration. This phase aims to gain insights and understand patterns within the data through statistical methods, descriptive analysis, and data visualization techniques. Exploratory data analysis helps uncover trends, correlations, and outliers, providing a deeper understanding of the dataset's characteristics.

Splitting the data into training and test datasets is crucial for building and evaluating predictive models. The training dataset is used to train the model, while the test dataset serves to assess its performance on unseen data. This step ensures that the model's performance can be generalized to new data and helps prevent overfitting.

Utilizing the XGBoost algorithm for training the model is a common approach in data science, especially for predictive modeling tasks. XGBoost is an efficient and scalable gradient boosting framework that excels in handling structured/tabular data and is widely used for classification and regression tasks.

Finally, after training the model on the training dataset, it is evaluated on the test dataset to assess its predictive performance. The model's ability to accurately predict loan defaults or non-defaults is measured using evaluation metrics such as accuracy, precision, recall, F1-score, etc. These metrics provide insights into the model's effectiveness and can inform decision-making processes in the banking domain.

In summary, the process of gathering, cleaning, analyzing, and modeling data from an international bank dataset is a fundamental workflow in data science. It involves various steps and techniques aimed at extracting actionable insights and building predictive models to support decision-making processes in the banking industry.

------

- ## Project 5 : Salary Estimation for Data Positions in the USA Public

This project focuses on estimating salaries utilizing both linear regression and classification models. It encompasses the following key stages:

## Data Importing and Cleaning from Glassdoor:
This initial step involves gathering relevant data from Glassdoor, a popular platform for job listings and employer reviews. The collected data undergoes cleaning processes to address any inconsistencies, missing values, or errors, ensuring the dataset's integrity for subsequent analysis.

## Exploratory Data Analysis (EDA):
EDA is a crucial phase where the collected data is explored and analyzed to gain insights into its structure, distributions, relationships, and potential patterns. Through techniques such as summary statistics, data visualization, and correlation analysis, key characteristics and trends within the dataset are identified. EDA helps in forming hypotheses and guiding subsequent modeling decisions.

## Feature Selection:
Feature selection involves identifying the most relevant and informative variables or features from the dataset that significantly contribute to predicting salaries. This process helps in reducing dimensionality, mitigating overfitting, and improving model performance by focusing on the most influential factors.

## Data Preprocessing:**
Data preprocessing encompasses various tasks such as handling missing values, encoding categorical variables, scaling numerical features, and potentially transforming data to meet modeling requirements. Preprocessing ensures that the data is in a suitable format for training machine learning models and enhances their interpretability and generalization capabilities.

## Modeling and Evaluation:
In this phase, different machine learning models, including linear regression and classification algorithms, are trained on the preprocessed data to predict salaries. Models are evaluated using appropriate metrics such as mean squared error (MSE), accuracy, precision, recall, or F1-score, depending on the nature of the problem (regression or classification). Evaluation helps in assessing model performance and selecting the best-performing model for further refinement.

## Fine-Tuning:
Fine-tuning involves optimizing model hyperparameters and configurations to improve their performance further. Techniques such as grid search, random search, or Bayesian optimization are employed to systematically explore the hyperparameter space and identify the optimal settings that yield the best model performance.

## rediction and Deployment:
Once the model is trained and fine-tuned, it is deployed to make predictions on new, unseen data. This deployed model can be integrated into production systems or used to provide interactive tools for users to explore salary predictions for data positions in the USA. Continuous monitoring and maintenance ensure that the deployed model remains accurate and relevant over time.
